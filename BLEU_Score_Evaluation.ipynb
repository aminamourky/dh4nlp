{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Llama-Instruct 7b Evaluation - BLEU Score\n",
        "The BLEU (Bilingual Evaluation Understudy) score is a metric commonly used to evaluate the quality of machine-generated text,\n",
        "particularly in the context of natural language generation tasks like code summarization. In this notebook, we use the BLEU\n",
        "score to evaluate the quality of code generated by the CodeLlama - Instruct 7b model in response to natural language prompts from the\n",
        "CoNaLa dataset. <br>\n",
        "Here's how the evaluation process works:\n",
        "1. We load the CoNaLa dataset, which contains natural language prompts (intents) and corresponding code snippets.\n",
        "2. We load the pre-trained CodeLlama - Instruct 7b model pipeline, which is specifically fine-tuned for code generation tasks following natural language instructions.\n",
        "3. We iterate through the testing set of the dataset, generating code snippets based on the provided intents using the model.\n",
        "4. For each generated code snippet, we calculate its BLEU score against the reference code snippet from the dataset.\n",
        "5. Finally, we compute the average BLEU score across all generated code snippets to evaluate the overall performance of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "gqAlfDaRnB0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T11:14:45.865352Z",
          "iopub.execute_input": "2024-03-18T11:14:45.865783Z",
          "iopub.status.idle": "2024-03-18T11:14:51.747423Z",
          "shell.execute_reply.started": "2024-03-18T11:14:45.865748Z",
          "shell.execute_reply": "2024-03-18T11:14:51.746595Z"
        },
        "trusted": true,
        "id": "y8M79sEtmX6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CoNaLa dataset\n",
        "dataset = load_dataset(\"neulab/conala\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T11:14:51.748567Z",
          "iopub.execute_input": "2024-03-18T11:14:51.749384Z",
          "iopub.status.idle": "2024-03-18T11:14:53.827104Z",
          "shell.execute_reply.started": "2024-03-18T11:14:51.749348Z",
          "shell.execute_reply": "2024-03-18T11:14:53.826214Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "07c461524ad9470abeef9cdfb638348d"
          ]
        },
        "id": "DBUrr2tYmX6a",
        "outputId": "f96078c7-8b12-4e59-f686-322bafba44a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07c461524ad9470abeef9cdfb638348d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset information\n",
        "print(dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T11:14:53.828220Z",
          "iopub.execute_input": "2024-03-18T11:14:53.828475Z",
          "iopub.status.idle": "2024-03-18T11:14:53.833876Z",
          "shell.execute_reply.started": "2024-03-18T11:14:53.828454Z",
          "shell.execute_reply": "2024-03-18T11:14:53.832976Z"
        },
        "trusted": true,
        "id": "MnxvBttemX6b",
        "outputId": "24fc3172-1585-4b46-bb73-1188ea9ba542"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['question_id', 'intent', 'rewritten_intent', 'snippet'],\n        num_rows: 2379\n    })\n    test: Dataset({\n        features: ['question_id', 'intent', 'rewritten_intent', 'snippet'],\n        num_rows: 500\n    })\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T11:14:53.835886Z",
          "iopub.execute_input": "2024-03-18T11:14:53.836158Z",
          "iopub.status.idle": "2024-03-18T11:14:53.842236Z",
          "shell.execute_reply.started": "2024-03-18T11:14:53.836135Z",
          "shell.execute_reply": "2024-03-18T11:14:53.841310Z"
        },
        "trusted": true,
        "id": "mpCiCfNkmX6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline for Code Llama\n",
        "model = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"codellama/CodeLlama-7b-Instruct-hf\",\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"  # Use GPU if available\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T11:14:53.843247Z",
          "iopub.execute_input": "2024-03-18T11:14:53.843504Z",
          "iopub.status.idle": "2024-03-18T11:16:00.610467Z",
          "shell.execute_reply.started": "2024-03-18T11:14:53.843482Z",
          "shell.execute_reply": "2024-03-18T11:16:00.609596Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "bb9349ce8250435ba79c1ee720f41a42"
          ]
        },
        "id": "9vCcim91mX6d",
        "outputId": "c5cddf95-f7ae-4036-b47b-7e0a4d1b4de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-18 11:14:54.716872: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-18 11:14:54.716928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-18 11:14:54.720586: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9349ce8250435ba79c1ee720f41a42"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through testing set to generate code and calculate BLEU score\n",
        "bleu_scores = []\n",
        "for example in dataset[\"test\"]:\n",
        "\n",
        "    # Extract the natural language prompt (intent)\n",
        "    prompt = example[\"intent\"]\n",
        "\n",
        "    # Reference code for BLEU score calculation\n",
        "    reference_code = example[\"snippet\"]  # Expected code snippet\n",
        "\n",
        "    # Generate code using the model pipeline\n",
        "    output = model(prompt, max_length=50)\n",
        "    generated_code = output[0].get(\"generated_text\")  # Access generated code\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu = sentence_bleu([reference_code.split()], generated_code.split())\n",
        "    bleu_scores.append(bleu)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1yn0_4zWmX6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print average BLEU score\n",
        "print(f\"Average BLEU score: {sum(bleu_scores) / len(bleu_scores)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T12:24:38.307779Z",
          "iopub.execute_input": "2024-03-18T12:24:38.308479Z",
          "iopub.status.idle": "2024-03-18T12:24:38.313166Z",
          "shell.execute_reply.started": "2024-03-18T12:24:38.308447Z",
          "shell.execute_reply": "2024-03-18T12:24:38.312273Z"
        },
        "trusted": true,
        "id": "o_EyIMtumX6g",
        "outputId": "86476667-fee0-4474-8dfe-aacffae67b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Average BLEU score: 0.0952474061356062\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU score ranges from 0 to 1, with higher scores indicating better quality and similarity between the generated and\n",
        "reference code snippets. However, it's important to note that the BLEU score has limitations, such as being sensitive to\n",
        "lexical similarity and not capturing semantic equivalence perfectly. Therefore, while BLEU score provides a quantitative\n",
        "measure of performance, it should be interpreted alongside qualitative assessments and domain-specific considerations.\n"
      ],
      "metadata": {
        "id": "-LQeZPmzn1ED"
      }
    }
  ]
}